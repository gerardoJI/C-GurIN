{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función princiapal cgurin_responde\n",
    "### Sección 1: Descripción General de la Función\n",
    "\n",
    "La función cgurin_responde está diseñada para manejar consultas sobre seguridad minera subterránea. A través de una serie de pasos, procesa la consulta del usuario, identifica el nivel relevante de la mina, y luego busca información pertinente desde un índice vectorial en Pinecone. Dependiendo de la naturaleza de la consulta, devuelve planos de la mina o respuestas detalladas sobre emergencias y condiciones en la mina.\n",
    "\n",
    "Parámetros:\n",
    "- query: la consulta del usuario.\n",
    "- emergencia (opcional): información sobre una posible emergencia.\n",
    "- last_update (opcional): el estado más reciente de los sensores de la mina.\n",
    "- Valor de Retorno:\n",
    "- Una cadena de texto que contiene la respuesta generada, que puede ser un archivo de imagen o una respuesta textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Análisis de la Función cgurin_responde\n",
    "\n",
    "# Este notebook está diseñado para desglosar y explicar la función `cgurin_responde`, que se utiliza para responder a consultas relacionadas con la seguridad en la minería subterránea, haciendo uso de Pinecone, OpenAI y datos de sensores de la mina.\n",
    "\n",
    "# ## Importación de librerías necesarias\n",
    "# Primero, se importan las librerías que se utilizarán en la función. Algunas de ellas, como `os` y `dotenv`, son útiles para cargar variables de entorno y gestionar configuraciones de API.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import re\n",
    "import base64\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# ## Carga de variables de entorno\n",
    "\n",
    "# La función empieza cargando las variables de entorno desde un archivo `.env`, lo cual es fundamental para obtener las claves de acceso a los servicios de Pinecone y OpenAI. Si alguna clave falta, se lanza un error.\n",
    "\n",
    "# Cargar archivo .env con las claves de API\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la clave API de Pinecone\n",
    "api_key_pinecone = os.getenv(\"api_key_pinecone\")\n",
    "if not api_key_pinecone:\n",
    "    raise ValueError(\"La variable de entorno 'api_key_pinecone' no está definida en el archivo .env.\")\n",
    "\n",
    "# Inicializar Pinecone con la clave API\n",
    "pc = Pinecone(api_key=api_key_pinecone)\n",
    "\n",
    "# Obtener la clave API de OpenAI\n",
    "api_key_openai = os.getenv(\"api_key_openai\")\n",
    "if not api_key_openai:\n",
    "    raise ValueError(\"La variable de entorno 'api_key_openai' no está definida en el archivo .env.\")\n",
    "\n",
    "# Configura la clave API de OpenAI\n",
    "client = OpenAI(api_key=api_key_openai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sección 2: Procesamiento de la Consulta\n",
    "Se convierte la consulta a minúsculas para normalizar la entrada y facilitar la búsqueda.\n",
    "Se definen los niveles de la mina en una lista (niveles).\n",
    "A través de una expresión regular, se extrae el número de nivel que aparece en la consulta, si existe. Si no se encuentra un número, se asigna el valor \"general\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la consulta a minúsculas para normalizar la entrada\n",
    "query_lower = query.lower()\n",
    "\n",
    "# Definir los niveles disponibles en la mina\n",
    "niveles = [\"1950\", \"1850\", \"1810\", \"1750\", \"1900\", \"general\"]\n",
    "\n",
    "# Extraer el número de nivel de la consulta utilizando una expresión regular\n",
    "number_in_query = re.search(r'\\d+', query)\n",
    "nivel = number_in_query.group() if number_in_query else \"general\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sección 3: Búsqueda de Información sobre Planos\n",
    "Explicación:\n",
    "- Si la consulta está relacionada con planos de la mina, se realiza una consulta a OpenAI para obtener un embedding de la consulta. Este embedding es luego utilizado para realizar una búsqueda en Pinecone, que devuelve los resultados más relevantes.\n",
    "- La búsqueda en Pinecone filtra por imágenes codificadas en base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso en que la consulta busca información sobre un plano de algún nivel\n",
    "if ('plano' in query_lower or 'lageplan' in query_lower or 'plane' in query_lower) and any(nivel in query_lower for nivel in niveles):\n",
    "    # Llamada a OpenAI para obtener el embedding de la consulta\n",
    "    response_imagen = client.embeddings.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    # Realizar la consulta al índice vectorial de Pinecone para buscar resultados de planos\n",
    "    results_imagen = index.query(\n",
    "        namespace=\"ns1\",\n",
    "        vector=response_imagen.data[0].embedding,\n",
    "        top_k=3,  # Número de resultados a devolver\n",
    "        include_values=True,\n",
    "        include_metadata=True,\n",
    "        filter={\"imagen_b64\": {\"$ne\": None, \"$ne\": '', \"$ne\": \" \"}}  # Filtrar por aquellos que tengan imagen en base64\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sección 4: Decodificación de Imágenes y Respuesta\n",
    "- Los resultados de la búsqueda son filtrados para asegurarse de que coincidan con el nivel específico y que contengan la palabra \"plano\".\n",
    "- Si se encuentran resultados válidos, la imagen en base64 es decodificada y convertida a un archivo temporal de imagen, que luego es retornado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los resultados para asegurarse de que coincidan con el nivel y contengan \"plano\"\n",
    "filtered_results = []\n",
    "if nivel:  # Verificar si 'nivel' tiene un valor válido antes de realizar el filtrado\n",
    "    for match in results_imagen['matches']:\n",
    "        if nivel in match['id'] and 'plano' in match['id']:\n",
    "            filtered_results.append(match)\n",
    "\n",
    "# Verificar si hay resultados filtrados\n",
    "if filtered_results:\n",
    "    # Extraer la imagen en base64 del primer resultado filtrado\n",
    "    imagen_b64 = filtered_results[0][\"metadata\"].get(\"imagen_b64\", \"\")\n",
    "    \n",
    "    if imagen_b64:  # Solo proceder si se encuentra una imagen en base64 válida\n",
    "        # Decodificar la imagen base64\n",
    "        imagen_data = base64.b64decode(imagen_b64)\n",
    "        \n",
    "        # Convertir a formato de imagen con PIL\n",
    "        image = Image.open(BytesIO(imagen_data))\n",
    "        \n",
    "        # Crear un archivo temporal y guardar la imagen\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as temp_file:\n",
    "            image.save(temp_file, format=\"PNG\")  # Guardar la imagen en el archivo temporal\n",
    "            temp_file_path = temp_file.name  # Obtener la ruta del archivo temporal\n",
    "        \n",
    "        # Retornar la ruta del archivo temporal\n",
    "        return temp_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sección 5: Respuesta para Consultas Generales o Emergencias\n",
    "- Si la consulta no está relacionada con un plano, se busca información textual utilizando Pinecone y OpenAI.\n",
    "- Se filtran los resultados para asegurarse de que no contengan imágenes.\n",
    "- Se prepara un mensaje para OpenAI con información adicional si la consulta es sobre una emergencia, y se obtiene una respuesta generada por el modelo GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    # Si la consulta no es sobre un plano, se procede a consultar la información textual\n",
    "    response = client.embeddings.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    # Realizar la consulta al índice vectorial de Pinecone para buscar información relevante\n",
    "    results = index.query(\n",
    "        namespace=\"ns1\",\n",
    "        vector=response.data[0].embedding,\n",
    "        top_k=5,  # Número de resultados a devolver\n",
    "        include_values=False,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    # Filtrar los resultados para asegurar que el 'nivel' esté en el 'id' y que no contengan imágenes\n",
    "    filtered_results = []\n",
    "    for match in results['matches']:\n",
    "        imagen_b64 = match['metadata'].get(\"imagen_b64\", \"\")\n",
    "        if imagen_b64 in [None, \"\", \" \"]:  # Solo considerar resultados sin imagen\n",
    "            filtered_results.append(match)\n",
    "\n",
    "    # Si hay resultados filtrados, preparar el mensaje para el modelo GPT\n",
    "    if filtered_results:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",  # Mensaje del sistema con información básica sobre el contexto\n",
    "                \"content\": \"Eres un experto en seguridad de minería subterránea...\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",  # Mensaje del usuario con la consulta\n",
    "                \"content\": f\"El usuario te dará un mensaje como este: \\\"{query}\\\".\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Si el diccionario 'emergencia' se proporciona, agregarlo al mensaje\n",
    "        if emergencia:\n",
    "            messages.append({\n",
    "                \"role\": \"user\",  # Mensaje adicional con detalles de la emergencia\n",
    "                \"content\": f\"Información adicional sobre la emergencia: {emergencia}\"\n",
    "            })\n",
    "\n",
    "        # Llamada a OpenAI para obtener una respuesta basada en el modelo GPT-4\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  # Asegúrate de que estés utilizando el modelo adecuado\n",
    "            messages=messages,\n",
    "            temperature=0.9,\n",
    "            max_tokens=2048\n",
    "        )\n",
    "\n",
    "        # Almacenar y retornar la respuesta generada por GPT\n",
    "        respuesta = response.choices[0].message.content\n",
    "        return respuesta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgurin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
